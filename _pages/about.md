---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


I'm currently studying at Huazhong University of Science and Technology, under the supervision of Prof. [Xiang Bai](https://scholar.google.com/citations?user=UeltiQ4AAAAJ&hl=en).  


Research Interests
------
* 3D Multimodal Large Language Models (3D MLLMs)
* Embodied AI
* 3D Vision: Point cloud analysis


üî• News
------
* [2025/11/8] üéâ One paper is accepted by AAAI2026 as oral presentation!
* [2024/4/20] Open personal website.

üìù Publicaitons
------
<span style="font-size: 0.85em; color: #666;">* Equal Contribution, ‚Ä† Corresponding Author</span>

<div class="publication-grid" style="display: grid; grid-template-columns: minmax(220px,320px) 1fr; gap: 20px; margin-bottom: 30px; align-items: start;">
  <div class="publication-img" style="text-align: center; display:flex; align-items:center; justify-content:center;">
    <img class="publication-thumb" src="/images/publications/toc3d.png" alt="toc3d" />
  </div>
  <div>
    <h4 style="margin-top: 0;">Make Your ViT-Based Multi-view 3D Detectors Faster via Token Compression</h4>
    <p style="color: #000; font-style: italic; margin: 5px 0;"> ECCV 2024</p>
    <p style="color: #666; margin: 5px 0; font-size: 0.9em;">Dingyuan Zhang, Dingkang Liang, Zichang Tan, Xiaoqing Ye, <strong style="color: #000;">Cheng Zhang</strong>, Jingdong Wang, Xiang Bai</p>
    <p style="margin: 10px 0;">An efficient sparse query-based multi-view 3D detector for autonomous driving.</p>
    <div style="display:flex; gap:12px; align-items:center; margin-top:10px;">
      <a href="#" title="Project" style="color: #1976d2; text-decoration: none; font-size: 0.95em;"><i class="fas fa-globe" aria-hidden="true"></i></a>
      <a href="https://link.springer.com/chapter/10.1007/978-3-031-72970-6_4" title="PDF" style="color: #1976d2; text-decoration: none; font-size: 0.95em;"><i class="fas fa-file-pdf" aria-hidden="true"></i></a>
      <a href="https://github.com/DYZhang09/ToC3D" class="github-link" data-repo="chenfengxu714/StreamDiffusionV2" title="GitHub" style="color: #1976d2; text-decoration: none; font-size: 0.95em;"><i class="fab fa-github" aria-hidden="true"></i> </a>
    </div>
  </div>
</div>

<div class="publication-grid" style="display: grid; grid-template-columns: minmax(220px,320px) 1fr; gap: 20px; margin-bottom: 30px; align-items: start;">
  <div class="publication-img" style="text-align: center; display:flex; align-items:center; justify-content:center;">
    <img class="publication-thumb" src="/images/publications/MMATrans.png" alt="toc3d" />
  </div>
  <div>
    <h4 style="margin-top: 0;">MMATrans: Muscle Movement Aware Representation Learning for Facial Expression Recognition via Transformers</h4>
    <p style="color: #000; font-style: italic; margin: 5px 0;">IEEE Transactions on Industrial Informatics (TII), 2024</p>
    <p style="color: #666; margin: 5px 0; font-size: 0.9em;">Hai Liu, Qiyun Zhou, <strong style="color: #000;">Cheng Zhang</strong>, Junyan Zhu, Tingting Liu, Zhaoli Zhang, You-Fu Li</p>
    <div style="display:flex; gap:12px; align-items:center; margin-top:10px;">
      <a href="https://ieeexplore.ieee.org/document/10636220" title="PDF" style="color: #1976d2; text-decoration: none; font-size: 0.95em;"><i class="fas fa-file-pdf" aria-hidden="true"></i></a>
    </div>
  </div>
</div>

<div class="publication-grid" style="display: grid; grid-template-columns: minmax(220px,320px) 1fr; gap: 20px; margin-bottom: 30px; align-items: start;">
  <div class="publication-img" style="text-align: center; display:flex; align-items:center; justify-content:center;">
    <img class="publication-thumb" src="/images/publications/tip23.png" alt="toc3d" />
  </div>
  <div>
    <h4 style="margin-top: 0;">Orientation Cues-aware Facial Relationship Representation for Head Pose Estimation via Transformer</h4>      <a href="https://ieeexplore.ieee.org/document/10318055" title="PDF" style="color: #1976d2; text-decoration: none; font-size: 0.95em;"><i class="fas fa-file-pdf" aria-hidden="true"></i></a>
    <p style="color: #000; font-style: italic; margin: 5px 0;">IEEE Transactions on Image Processing (TIP), 2023</p>
    <p style="color: #666; margin: 5px 0; font-size: 0.9em;">Hai Liu, <strong style="color: #000;">Cheng Zhang<sup>‚Ä†</sup></strong>, Yongjian Deng, Tingting Liu, Zhaoli Zhang, You-Fu Li<sup>‚Ä†</sup></p>
    <div style="display:flex; gap:12px; align-items:center; margin-top:10px;">

    </div>
  </div>
</div>


<div class="publication-grid" style="display: grid; grid-template-columns: minmax(220px,320px) 1fr; gap: 20px; margin-bottom: 30px; align-items: start;">
  <div class="publication-img" style="text-align: center; display:flex; align-items:center; justify-content:center;">
    <img class="publication-thumb" src="/images/publications/tokenhpe.png" alt="toc3d" />
  </div>
  <div>
    <h4 style="margin-top: 0;">TokenHPE: Learning Orientation Tokens for Efficient Head Pose Estimation via Transformers</h4>
    <p style="color: #000; font-style: italic; margin: 5px 0;">CVPR 2023</p>
    <p style="color: #666; margin: 5px 0; font-size: 0.9em;"><strong style="color: #000;">Cheng Zhang</strong>, Hai Liu, Yongjian Deng, Bochen Xie, Youfu Li</p>
    <div style="display:flex; gap:12px; align-items:center; margin-top:10px;">
      <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_TokenHPE_Learning_Orientation_Tokens_for_Efficient_Head_Pose_Estimation_via_CVPR_2023_paper.html" title="PDF" style="color: #1976d2; text-decoration: none; font-size: 0.95em;"><i class="fas fa-file-pdf" aria-hidden="true"></i></a>
    </div>
  </div>
</div>

<div class="publication-grid" style="display: grid; grid-template-columns: minmax(220px,320px) 1fr; gap: 20px; margin-bottom: 30px; align-items: start;">
  <div class="publication-img" style="text-align: center; display:flex; align-items:center; justify-content:center;">
    <img class="publication-thumb" src="/images/publications/transifc.png" alt="toc3d" />
  </div>
  <div>
    <h4 style="margin-top: 0;">TransIFC: Invariant Cues-aware Feature Concentration Learning for Efficient Fine-grained Bird Image Classification</h4>
    <p style="color: #000; font-style: italic; margin: 5px 0;">IEEE Transactions on Multimedia (TMM), 2023</p>
    <p style="color: #666; margin: 5px 0; font-size: 0.9em;">Hai Liu, <strong style="color: #000;">Cheng Zhang<sup>‚Ä†</sup></strong>, Yongjian Deng, Bochen Xie, Tingting Liu, Zhaoli Zhang, You-Fu Li<sup>‚Ä†</sup></p>
    <div style="display:flex; gap:12px; align-items:center; margin-top:10px;">
      <a href="https://ieeexplore.ieee.org/abstract/document/10023961" title="PDF" style="color: #1976d2; text-decoration: none; font-size: 0.95em;"><i class="fas fa-file-pdf" aria-hidden="true"></i></a>
    </div>
  </div>
</div>

<div class="publication-grid" style="display: grid; grid-template-columns: minmax(220px,320px) 1fr; gap: 20px; margin-bottom: 30px; align-items: start;">
  <div class="publication-img" style="text-align: center; display:flex; align-items:center; justify-content:center;">
    <img class="publication-thumb" src="/images/publications/robio.png" alt="toc3d" />
  </div>
  <div>
    <h4 style="margin-top: 0;">Affinity Relation-aware Fine-grained Bird Image Recognition for Robot Vision Tracking via Transformers</h4>
    <p style="color: #000; font-style: italic; margin: 5px 0;">IEEE International Conference on Robotics and Biomimetics (ROBIO), 2022</p>
    <p style="color: #666; margin: 5px 0; font-size: 0.9em;">Hai Liu, <strong style="color: #000;">Cheng Zhang<sup>‚Ä†</sup></strong>, Bochen Xie, Tingting Liu, Qingsong Xu, You-Fu Li<sup>‚Ä†</sup></p>
    <div style="display:flex; gap:12px; align-items:center; margin-top:10px;">
      <a href="https://ieeexplore.ieee.org/abstract/document/10011861" title="PDF" style="color: #1976d2; text-decoration: none; font-size: 0.95em;"><i class="fas fa-file-pdf" aria-hidden="true"></i></a>
    </div>
  </div>
</div>

<style>
.gh-stars{font-weight:600; margin-left:6px; color:#333; font-size:0.95em}
.github-link i{margin-right:4px}

/* Publication layout tweaks */
.publication-grid { grid-auto-rows: auto; }
.publication-img img { display:block; }

/* Image container: use cover so images are visually uniform */
.publication-img { width:320px; height:220px; overflow:hidden; border-radius:8px; position:relative; display:flex; align-items:center; justify-content:center; }
.publication-img img.publication-thumb { width:100%; height:100%; object-fit:cover; object-position:center; }

/* Compact icon links (small, no heavy background) */
a[title]{ color: #1976d2; text-decoration: none; font-size: 0.95em; }

/* Widen the content column for this page and normalize typography to match resume style */
.page__content { max-width: 980px; margin-left: auto; margin-right: auto; }
.page__content h2 { font-size: 1.4rem !important; margin-top: 1.1em; }
.page__content h3 { font-size: 1.15rem !important; margin-top: 1em; }
.page__content h4 { font-size: 1.05rem !important; margin-top: 0.6em; }
.page__content p, .page__content li, .page__content dd { font-size: 0.95rem !important; color: #333; line-height: 1.5; }

/* Research/Honors grid helper classes (used below) */
.research-grid, .honors-grid { display: grid; grid-template-columns: 1fr auto; gap: 15px; align-items: start; margin-bottom: 12px; }
.research-grid .meta, .honors-grid .meta { text-align: right; white-space: nowrap; font-size: 0.9em; color: #666; }

@media (max-width: 900px) {
  .publication-grid { grid-template-columns: 1fr !important; }
  .publication-img { width: 100%; height: 200px; }
  .page__content { padding: 0 1rem; max-width: 100%; }
}
</style>

<script>
/*
  Robust GitHub star fetcher with localStorage caching.
  - Caches results 6 hours to avoid hitting unauthenticated rate limits
  - Fills all elements with class 'gh-stars' (reads data-repo attribute)
  - Falls back to a stale cached value or '‚Äì' on errors
*/
(async function(){
  function cacheGet(key, maxAgeMs){
    try{
      const raw = localStorage.getItem(key);
      if(!raw) return null;
      const obj = JSON.parse(raw);
      if(Date.now() - obj.t > maxAgeMs) { localStorage.removeItem(key); return null; }
      return obj.v;
    }catch(e){ return null; }
  }
  function cacheSet(key, value){
    try{ localStorage.setItem(key, JSON.stringify({t: Date.now(), v: value})); }catch(e){}
  }

  async function fetchStars(repo){
    const cacheKey = 'ghstars:' + repo;
    const cached = cacheGet(cacheKey, 1000*60*60*6); // 6 hours
    if(cached !== null) return cached;
    try{
      const res = await fetch('https://api.github.com/repos/' + repo, {headers:{'Accept':'application/vnd.github.v3+json'}});
      if(!res.ok) {
        const stale = cacheGet(cacheKey, Number.MAX_SAFE_INTEGER);
        return stale !== null ? stale : '‚Äì';
      }
      const j = await res.json();
      const count = (j.stargazers_count || 0).toLocaleString();
      cacheSet(cacheKey, count);
      return count;
    }catch(e){
      const stale = cacheGet(cacheKey, Number.MAX_SAFE_INTEGER);
      return stale !== null ? stale : '‚Äì';
    }
  }

  function render(el, text){ if(!el) return; el.textContent = '‚òÖ ' + text; }

  function run(){
    const els = document.querySelectorAll('.gh-stars');
    if(!els || els.length === 0) return;
    const repoMap = {};
    els.forEach(el => {
      const repo = el.dataset.repo;
      if(repo) {
        if(!repoMap[repo]) repoMap[repo] = [];
        repoMap[repo].push(el);
      }
    });
    Object.keys(repoMap).forEach(async repo => {
      repoMap[repo].forEach(el => { el.textContent = '‚òÖ ‚Ä¶'; });
      const count = await fetchStars(repo);
      repoMap[repo].forEach(el => render(el, count));
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', run);
  } else {
    run();
  }
})();
</script>



* [Make Your ViT-Based Multi-view 3D Detectors Faster via Token Compression](https://link.springer.com/chapter/10.1007/978-3-031-72970-6_4), Dingyuan Zhang, Dingkang Liang, Zichang Tan, Xiaoqing Ye, **Cheng Zhang**, Jingdong Wang, Xiang Bai, *European Conference on Computer Vision (ECCV)*, 2024
* [MMATrans: Muscle Movement Aware Representation Learning for Facial Expression Recognition via Transformers](https://ieeexplore.ieee.org/document/10636220), Hai Liu, 
Qiyun Zhou, **Cheng Zhang**, Junyan Zhu, Tingting Liu; Zhaoli Zhang, You-Fu Li, *IEEE Transactions on Industrial Informatics (TII)*, 2024
* [Orientation Cues-aware Facial Relationship Representation for Head Pose Estimation via Transformer](https://ieeexplore.ieee.org/document/10318055), Hai Liu; **Cheng Zhang**; Yongjian Deng; Tingting Liu; Zhaoli Zhang; You-Fu Li, *IEEE Transactions on Image Processing (TIP)*, 2023
* [TokenHPE: Learning Orientation Tokens for Efficient Head Pose Estimation via Transformers](https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_TokenHPE_Learning_Orientation_Tokens_for_Efficient_Head_Pose_Estimation_via_CVPR_2023_paper.html), **Cheng Zhang**, Hai Liu, Yongjian Deng, Bochen Xie, Youfu Li, *Conference on Computer Vision and Pattern Recognition (CVPR)*, 2023
* [TransIFC: Invariant Cues-aware Feature Concentration Learning for Efficient Fine-grained Bird Image Classification](https://ieeexplore.ieee.org/abstract/document/10023961), Hai Liu; **Cheng Zhang**; Yongjian Deng; Bochen Xie; Tingting Liu; Zhaoli Zhang; You-Fu Li, *IEEE Transactions on Multimedia (TMM)*, 2023
* [Affinity Relation-aware Fine-grained Bird Image Recognition for Robot Vision Tracking via Transformers](https://ieeexplore.ieee.org/abstract/document/10011861), Hai Liu; **Cheng Zhang**; Bochen Xie; Tingting Liu; Qingsong Xu; You-Fu Li, *IEEE International Conference on Robotics and Biomimetics (ROBIO)*, 2022

üèÖ Honors and Awards
------
2023.10, National Scholarship (Undergraduate)

üìñ Educations
------
* 2024.09-2027.06 (expected), Master degree, Huazhong University of Science and Technology. Supervised by Prof. [Xiang Bai](https://scholar.google.com/citations?user=UeltiQ4AAAAJ&hl=en).
* 2024.06, Bachelor of Engineering in Artificial Intelligence, Central China Normal University. Supervised by Prof. [Hai Liu](https://scholar.google.com/citations?user=Tjw7zHEAAAAJ&hl=en).
* IELTS: 7.5 (Listening-8.0; Speaking-6.0; Reading-8.5; Writing-6.5), 04/2023
* GRE: 324 (Verbal-155; Quantitative-169; Analytical Writing-3.0), 07/2023
* CET 6: 587, 05/2022


Internship
------
I am now available on the job market.



---

<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=4yP6BEk3dPm3WM4dSyHf6UzOTTNUv23thzo95DQTAsw&co=a8e6ff&cmo=3acc3a&cmn=ff5353&ct=000000'></script>

---

<p align="center">
  <img src="https://count.getloli.com/@zc2023.github.io?name=zc2023.github.io&theme=moebooru&padding=7&offset=0&align=center&scale=0.85&pixelated=1&darkmode=auto">
</p>


